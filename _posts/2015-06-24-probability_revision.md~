---
layout: post
title: "Probability Revision"
description: ""
category: "Pro"
tags: [Probability]
---
{% include JB/setup %}

<!--more-->

##Math notations

$\sum_x f(x) \equiv \sum_{s \in dom(x)} f(x = s)$


**Definition 1.1** (Rules of Probability for Discrete Variables)

The summation of the probability over all the state is 1:

$\sum_{x \in dom(x)} p(x = x) = 1$

This is called normalisation condition. We write $\sum_x p(x) = 1$ for short.

Two variables $x$ and $y$ can interact through

$p(x, y) = p(x) + p(y) - p(x \cap y)$


**Definition 1.2 (Set notation) An alternative notation in terms of set theory is to write**

$p(\text{x or y}) \equiv p(x \cup y)$

$p(x, y) \equiv p(x \cap y)$

**Definition 1.3 (Marginals)**

$p(x) = \sum_y p(x, y)$

**Definition 1.4 (Conditional Probability / Bayes' Rule)**

$p(x \mid y) = \frac{p(y \mid x)p(x)}{p(y)}$

**Definition 1.5 (Probability Density Functions) For a continuous variable x, the probability density f(x) is defined
such that**

$f(x) \geq 0, \int_{-\infty}^\infty f(x) \mathrm{d}x = 1$

and the probability that $x$ falls in an interval [a, b] is given by

$p(a \leq x \leq b) = \int_a^b f(x) \mathrm{d}x$

Unlike probabilities, probability densities can take positive values greater than 1.

**Definition 1.6 (Independence)**

$p(x, y) = p(x)p(y)$

Provided that $p(x) \neq \text{0 and }p(y) \neq$0 independence of x and y is equivalent to

$p(x \mid y) = p(x)$

$\Leftrightarrow$

$p(y \mid x) = p(y)$

we write $x \bot y$

**Definition 1.7 (Conditional Independence)**

$X \bot Y \mid Z$

denotes that the two sets of variables $X$ and $Y$ are independent of each other provided we know the state of the set
of variables $Z$. For conditional independence, $X$ and $Y$ must be independent given all states of $Z$, means:

$p(X, Y \mid Z) = p(X \mid Z)p(y \mid Z)$

If $X$ and $Y$ are not conditionally independent, they are conditionally dependent, This is written:

$X \top Y \mid Z$

Similarly $X \top Y \mid \emptyset$ can be written as $X \top Y$


