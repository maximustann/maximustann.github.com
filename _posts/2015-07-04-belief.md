---
layout: post
title: "Belief Networks"
description: ""
category: "Mach"
tags: [Mach]
---
{% include JB/setup %}


<!--more-->

The only way to deal with large distributions is to constrain the nature
of the variable interactions in some manner. The key idea is to specify
which variables are independent of others, leading to a structured
factorisation of the joint probability distribution. 

Belief networks are a way to depict the independence assumptions made in
a distribution. Nevertheless, in expressing these independencies it can
be useful (though also potentially misleading) to think of 'what causes 
what'. 

**Reducing the burden of specification**

Consider a discrete variable $y$ with many discrete parental variables 
$x_1, \dots, x_n$. 

![Imgur](http://i.imgur.com/csIwOaA.png)

Formally, the structure of the graph implies nothing about the form of the 
parameterisation of the table $p(y \mid x_1, \dots, x_n)$. 
If each parent $x_i$ has dim($x_i$) states, and there is no constraint on
the table $p(y \mid x_1, \dots, x_n)$ contains (dim(y) - 1)$\prod_i$dim($x_i$) entries.

If stored explicitly for each state, this would require potentially huge 
storage. An alternative is to constrain the table to have a simpler
parametric form. For example, one might write a decomposition in which
only a limited number of parental interactions are required (this is 
called *divorcing parents*) For example, 

![Imgur](http://i.imgur.com/tLehGZR.png)

we have

$p(y \mid x_1, \dots, x_5) = \sum_{z_1, z_2} p(y \mid z_1, z_2)p(z_1\mid
x_1, x_2, x_3)p(z_2 \mid x_4, x_5)$

Assuming all variables are binary, the number of states requiring 
specification is $2^3 + 2^2 + 2^2 = 16$, compared to the $2^5 = 32$
states in the unconstrained case.

**Logic gates**

Another technique to constrain tables uses simple classes of conditional
tables. For example, 

![Imgur](http://i.imgur.com/75x59xi.png)

one could use a logic OR gate on binary $z_i$, say

![Imgur](http://i.imgur.com/w4PUXZ7.png)

We can then make a table $p(y \mid x_1, \dots, x_5)$ by including The
additional terms $p(z_i = 1 \mid x_i)$. When each $x_i$ is binary there
are in total only 2 + 2 + 2 + 2 + 2 = 10 quantities required fr specify 
$p(y \mid x)$. In this case, fig(3.2c) can be used to represent any
*noisy logic gate*, such as the *noisy* Or or *noisy* AND, where the 
number of parameters required to specify the noisy gate is linear in 
the number of parents.

The noisy-OR is particularly common in disease-symptom networks in 
which many diseases x can give rise to the same symptom $y$ provided 
that at least one of the disease is present, the probability that the 
symptom will be present is high.

---

###Uncertain and Unreliable Evidence

We make a distinction between evidence that is Uncertain, and evidence 
that is reliable.

**Uncertain evidence**

In soft or *uncertain evidence*, the evidence variable is in more than 
one state, with the strength of our belief about each state being 
given by probabilities. For example, if x has the states dom(x) = 
{red,blue,green} the vector (0.6, 0.1, 0.3) represents the belief in 
the respective states. In contrast, for *hard evidence* we are certain
that a variable is in a particular state. In this case, all the probability
mass is in one of the vector components, for example (0, 0, 1).

Performing inference with soft-evidence is straightforward and can be 
achieved using Bayes' rule. For example, for a mode $p(x, y)$, Consider
that we have some soft evidence $\tilde{y}$ about the variable $y$, 
and wish to know what effect this has on the variable $x$ that is 
we wish to compute $p(x\mid \tilde{y})$. From Bayes' rule, and 
the assumption $p(x \mid y, \tilde{y}) = p(x \mid y)$, we have

$p(x \mid \tilde{y}) = \sum_y p(x, y \mid \tilde{y}) = \sum_y p(x \mid y, \tilde{y}) p(y \mid \tilde{y}) = \sum_y p(x \mid y)p(y \mid \tilde{y})$

where $p(y = i \mid \tilde{y})$ represents the probability that $y$ is in 
state $i$ under the soft-evidence. This is a generalisation of hard-Evidence
in which the vector $p(y \mid \tilde{y})$ has all zero component values,
except for a single component. This procedure in which we first define 
the model conditional on the evidence, and then average over the 
distribution of the evidence is also knwon as Jeffrey's rule.

In the BN we use a dashed circle to represent that a variable is in 
a soft-evidence state. 


