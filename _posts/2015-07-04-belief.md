---
layout: post
title: "Belief Networks"
description: ""
category: "Mach"
tags: [Mach]
---
{% include JB/setup %}


<!--more-->

The only way to deal with large distributions is to constrain the nature
of the variable interactions in some manner. The key idea is to specify
which variables are independent of others, leading to a structured
factorisation of the joint probability distribution. 

Belief networks are a way to depict the independence assumptions made in
a distribution. Nevertheless, in expressing these independencies it can
be useful (though also potentially misleading) to think of 'what causes 
what'. 

**Reducing the burden of specification**

Consider a discrete variable $y$ with many discrete parental variables 
$x_1, \dots, x_n$. 

！[Imgur](http://i.imgur.com/csIwOaA.png)

Formally, the structure of the graph implies nothing about the form of the 
parameterisation of the table $p(y \mid x_1, \dots, x_n)$. 
If each parent $x_i$ has dim($x_i$) states, and there is no constraint on
the table $p(y \mid x_1, \dots, x_n)$ contains (dim(y) - 1)$\prod_i$dim($x_i$) entries.

If stored explicitly for each state, this would require potentially huge 
storage. An alternative is to constrain the table to have a simpler
parametric form. For example, one might write a decomposition in which
only a limited number of parental interactions are required (this is 
called *divorcing parents*) For example, 

！[Imgur](http://i.imgur.com/tLehGZR.png)

we have

$p(y \mid x_1, \dots, x_5) = \sum_{z_1, z_2} p(y \mid z_1, z_2)p(z_1\mid
x_1, x_2, x_3)p(z_2 \mid x_4, x_5)$

Assuming all variables are binary, the number of states requiring 
specification is $2^3 + 2^2 + 2^2 = 16$, compared to the $2^5 = 32$
states in the unconstrained case.
